schema_version: "1.0"
name: "AI Agent Decision Quality Evaluator"
version: "1.0"
description: "Evaluates AI agent decision-making quality and output appropriateness"

ingestion:
  type: "generic_otel"
  config:
    input_field: "attributes.user_problem"
    output_field: "attributes.agent_output"
    expected_output_field: "attributes.expected_behavior"
    span_kind_filter: []  # Accept all span types
    include_trace_context: true

pipeline:
  - name: "agent_quality_assessment"
    scorer: "llm_judge"
    config:
      provider: "openai"
      model: "gpt-4"
      threshold: 0.7
      system_prompt: |
        You are an expert at evaluating AI agent performance. Assess the agent's decision-making quality, reasoning clarity, and output appropriateness.

        Return a JSON object with:
        - "score": float 0-1 (quality rating)
        - "reasoning": detailed explanation
        - "strengths": list of what the agent did well
        - "improvements": list of suggested improvements
      user_prompt_template: |
        Evaluate this AI agent interaction:

        User Request: {input}
        Agent Response: {output}

        Full Context: {metadata}

        Assess based on:
        1. Did the agent understand the user's intent correctly?
        2. Was the reasoning process logical and transparent?
        3. Is the output helpful, accurate, and appropriate?
        4. Are there any safety or ethical concerns?
        5. Could the response be improved?

# These examples provide rich context for evaluating:
# - Multi-step reasoning chains
# - Decision-making transparency
# - Output quality and appropriateness
# - Problem-solving approaches
# - Creative task fulfillment
schema_version: "1.0"
name: "Legal Citations Comprehensive Evaluation"
version: "1.0"
description: "Multi-stage evaluation for legal citations with normalized matching and semantic LLM judgment"
author: "Lake Merritt Team"

ingestion:
  type: "csv"
  config:
    # The CSV already has 'output' column, so we use evaluate_existing mode
    mode: "evaluate_existing"

pipeline:
  # Stage 1: Strict exact match to identify perfect matches
  - name: "exact_match_baseline"
    scorer: "exact_match"
    config:
      case_sensitive: true
      normalize_whitespace: false
    on_fail: "continue"
  
  # Stage 2: Case-insensitive exact match
  - name: "case_insensitive_match"
    scorer: "case_insensitive_exact_match"
    on_fail: "continue"
  
  # Stage 3: Normalized match with punctuation tolerance
  - name: "normalized_match"
    scorer: "normalized_exact_match"
    config:
      ignore_trailing_punctuation: true
    on_fail: "continue"
  
  # Stage 4: Fuzzy match to catch minor variations
  - name: "fuzzy_match_check"
    scorer: "fuzzy_match"
    config:
      threshold: 0.9  # High threshold for legal citations
    on_fail: "continue"
  
  # Stage 5: LLM semantic judge for detailed analysis
  - name: "semantic_citation_judge"
    scorer: "llm_judge"
    config:
      provider: "openai"
      model: "gpt-4o"
      threshold: 0.7
      temperature: 0.1  # Low temperature for consistent scoring
      system_prompt: |
        You are an expert legal citation validator following The Bluebook citation format.
        Analyze the citations for both formatting accuracy and substantive correctness.
        Return a JSON object with:
        - "score": float between 0.0 and 1.0
        - "passed": boolean (true if score >= threshold)
        - "reasoning": detailed explanation of your evaluation
        - "errors": list of specific issues found (if any)
      user_prompt_template: |
        ### Task
        Compare the actual citation against the expected citation. Consider both:
        1. Substantive accuracy (case names, reporter, volume, page, year, court)
        2. Formatting compliance with Bluebook rules
        
        ### Scoring Guidelines
        - 1.0: Perfect match in both substance and format
        - 0.9: Correct substance, minor formatting differences (spacing, punctuation)
        - 0.8: Correct substance, notable but acceptable formatting variations
        - 0.7: Correct substance, significant formatting issues
        - 0.5: Minor substantive differences (e.g., abbreviated vs. full party names)
        - 0.3: Major substantive differences (wrong page, year, or court)
        - 0.0: Completely different case or fundamentally incorrect citation
        
        ### Common Acceptable Variations
        - "U.S." vs "US" (both acceptable)
        - Single vs. double spaces
        - Presence/absence of trailing periods
        - "Int'l" vs "International"
        - "Comm'r" vs "Commissioner"
        - En dash (â€“) vs hyphen (-) for page ranges
        
        ### Data to Evaluate
        **Expected Citation:** {expected_output}
        **Actual Citation:** {output}
        **Original Input:** {input}
    on_fail: "continue"

# Reporting configuration
reporting:
  format: "markdown"
  include_details: true
  summary_metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"

# Metadata
metadata:
  tags: ["legal", "citations", "bluebook", "multi-stage", "comprehensive"]
  domain: "legal"
  difficulty: "advanced"
  notes: |
    This evaluation pack provides comprehensive analysis of legal citations through
    multiple stages, from strict matching to semantic understanding. It's particularly
    useful for identifying both formatting inconsistencies and substantive errors
    in legal citation data.
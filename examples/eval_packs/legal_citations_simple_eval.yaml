schema_version: "1.0"
name: "Legal Citations Normalized + LLM Evaluation"
version: "1.0"
description: "Two-stage evaluation combining normalized exact match with LLM semantic judgment for legal citations"
author: "Lake Merritt Team"

ingestion:
  type: "csv"

pipeline:
  # Stage 1: Normalized exact match with formatting tolerance
  - name: "normalized_match"
    scorer: "normalized_exact_match"
    config:
      ignore_trailing_punctuation: true
    on_fail: "continue"
  
  # Stage 2: LLM judge for semantic analysis
  - name: "llm_citation_judge"
    scorer: "llm_judge"
    config:
      provider: "openai"
      model: "gpt-4o"
      threshold: 0.7
      temperature: 0.0  # Zero temperature for maximum consistency
      system_prompt: |
        You are a legal citation expert. Evaluate citations for substantive accuracy.
        Return JSON with "score" (0-1), "passed" (boolean), and "reasoning" (string).
      user_prompt_template: |
        Evaluate if these citations refer to the same legal authority and are properly formatted.
        
        Score as follows:
        - 1.0: Same case, perfect formatting
        - 0.8-0.9: Same case, minor formatting differences
        - 0.5-0.7: Same case, significant formatting issues
        - 0.0-0.4: Different case or wrong citation
        
        Expected: {expected_output}
        Actual: {output}
    on_fail: "continue"

metadata:
  tags: ["legal", "citations", "normalized", "llm"]
  estimated_runtime: "fast"
  api_costs: "low"
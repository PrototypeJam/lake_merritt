{
  "evaluation_config": {
    "eval_pack": {
      "schema_version": "1.0",
      "name": "Holistic Plan Improvement Score vs. Goal",
      "version": "1.0",
      "description": "Compares the initial vs. final plan strictly against the user's original goal to determine if the final version is more likely to succeed.\n",
      "author": null,
      "generation": null,
      "ingestion": {
        "type": "python",
        "parser": null,
        "config": {
          "script_path": "core/ingestion/agento_analytical_ingester.py",
          "entry_function": "ingest_agento_analytical_trace",
          "mode": "plan_delta"
        }
      },
      "pipeline": [
        {
          "name": "judge_holistic_plan_improvement",
          "scorer": "llm_judge",
          "config": {
            "provider": "openai",
            "model": "gpt-4o",
            "threshold": 0.75,
            "system_prompt": "You are a discerning executive who cares only about results. You have been given a user's goal, an initial project plan, and a final revised plan. You must ignore all other context. Your entire judgment must be based on one question: Which plan is more likely to fully and correctly achieve the user's stated goal?\n\nYou must return ONLY valid JSON with three fields: `rating` (an integer from 1-5), `rating_meaning` (the corresponding string description from the rubric), and your detailed `reasoning`.\n",
            "user_prompt_template": "### User's Original Goal:\n{{ input }}\n\n### Initial Plan Draft:\n{{ expected_output }}\n\n### Final Revised Plan:\n{{ output }}\n\n### Your Crystal Clear Task:\nUsing ONLY the User's Original Goal as your measure of success, evaluate if the Final Revised Plan is an improvement over the Initial Plan Draft. Use the following 5-point scale for your `rating`:\n\n- **5 (Significantly More Likely):** The final plan is a masterful improvement that is far more likely to achieve the user's goal. It addresses critical strategic flaws or adds essential components missing from the original.\n- **4 (Somewhat More Likely):** The final plan is a clear and definite improvement. Its structure, steps, or focus make it noticeably more likely to succeed.\n- **3 (About as Likely):** The changes are minor, cosmetic, or have little to no material impact on the probability of achieving the goal. Neither plan has a clear advantage.\n- **2 (Somewhat Less Likely):** The revision process was slightly detrimental. The final plan introduced some confusion, removed a good idea, or is less focused on the goal than the original.\n- **1 (Significantly Less Likely):** The final plan is a major regression. It is strategically flawed, incoherent, or has fundamentally misunderstood the user's goal compared to the initial draft.\n\nProvide your `rating`, the corresponding `rating_meaning` string, and your `reasoning`. Do not consider any other information besides the goal and the two plans."
          },
          "on_fail": "continue",
          "run_if": null,
          "span_kind": null
        }
      ],
      "reporting": null,
      "metadata": {}
    },
    "batch_size": 10,
    "privacy_settings": {}
  },
  "model_configs": {
    "default_judge_config": {
      "provider": "openai",
      "model": "gpt-4.1",
      "temperature": 0.3,
      "max_tokens": 1000,
      "system_prompt": "You are an expert evaluator. Compare the actual output to the expected output and provide:\n1. A score from 0.0 to 1.0 (where 1.0 is perfect match)\n2. A brief reasoning for your score\n3. Any specific errors or discrepancies noted\n\nRespond in JSON format:\n{\n    \"score\": 0.0-1.0,\n    \"reasoning\": \"explanation\",\n    \"errors\": [\"error1\", \"error2\"] or []\n}",
      "user_prompt_template": "Compare the actual output to the expected output for the given input.\n\nInput: {{ input }}\nExpected Output: {{ expected_output }}\nActual Output: {{ output }}\n\nRespond in JSON format with:\n- \"score\": 0.0 to 1.0\n- \"reasoning\": explanation of your evaluation"
    }
  },
  "selected_scorers": [],
  "timestamp": "20250731_062957"
}
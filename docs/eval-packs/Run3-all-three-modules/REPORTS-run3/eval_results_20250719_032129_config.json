{
  "evaluation_config": {
    "eval_pack": {
      "name": "Generalized Agento Lifecycle Evaluation",
      "version": "2.0",
      "description": "A reusable evaluation pack that judges the quality of each step in a multi-module Agento workflow by leveraging semantic OTEL attributes.\n",
      "author": null
    },
    "pipeline": [
      {
        "name": "judge_initial_plan",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.8,
          "system_prompt": "You are an expert project manager. Evaluate if the provided project plan is a sound and comprehensive response to the user's goal. Return JSON with \"score\" (0-1) and \"reasoning\".\n",
          "user_prompt_template": "### User's Goal:\n{input}\n### Agent's Generated Plan:\n{output}\n### Your Task: Evaluate the plan's quality and alignment with the goal.\n"
        },
        "on_fail": "continue",
        "span_kind": null
      },
      {
        "name": "judge_step_draft",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.7,
          "system_prompt": "You are an expert evaluator. Assess if the draft for step '{metadata[step_name]}' faithfully implements its instructions and criteria. Return JSON with \"score\" and \"reasoning\".\n",
          "user_prompt_template": "### Overall User Goal:\n{metadata[user_goal]}\n### Instructions & Criteria for this Step:\n{expected_output}\n### Agent's Draft for '{metadata[step_name]}':\n{output}\n### Your Task: Evaluate how well the draft fulfills its specific instructions.\n"
        },
        "on_fail": "continue",
        "span_kind": null
      },
      {
        "name": "judge_step_critique",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.8,
          "system_prompt": "You are an expert at providing constructive feedback. Evaluate if the critique for '{metadata[step_name]}' is insightful and actionable. Return JSON with \"score\" and \"reasoning\".\n",
          "user_prompt_template": "### Overall User Goal:\n{metadata[user_goal]}\n### Original Draft for '{metadata[step_name]}' (to be critiqued):\n{input}\n### Agent's Critique of the Draft:\n{output}\n### Your Task: Evaluate the quality of the critique. Is it valuable?\n"
        },
        "on_fail": "continue",
        "span_kind": null
      },
      {
        "name": "judge_accepted_revision",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.8,
          "system_prompt": "You are a quality assurance expert. Evaluate if the final revised content successfully and completely implements the requested revisions. Return JSON with \"score\" (0-1) and \"reasoning\".\n",
          "user_prompt_template": "### Step Name:\n{metadata[step_name]}\n### Original Revision Request:\n{input}\n### Final Accepted Content:\n{output}\n### Your Task: Did the final content successfully address the revision request?\n"
        },
        "on_fail": "continue",
        "span_kind": null
      },
      {
        "name": "judge_timed_out_revision",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.5,
          "system_prompt": "You are a senior agent architect analyzing a failed revision attempt. The process timed out after 3 iterations. Evaluate the final state. Return a JSON object with \"score\" (0-1, representing how promising the final state was), \"reasoning\", \"strengths\" of the last draft, and \"weaknesses\" of the final critique.\n",
          "user_prompt_template": "### Step Name:\n{metadata[step_name]}\n### Original Revision Request:\n{input}\n### Last Attempted Draft (before timeout):\n{output}\n### Final Critique (that caused the loop to continue):\n{metadata[final_critique]}\n### Your Task: Analyze this failed state. Was the last draft close to being acceptable? Was the final critique fair or was it stuck on minor issues? Provide a detailed analysis.\n"
        },
        "on_fail": "continue",
        "span_kind": null
      },
      {
        "name": "judge_holistic_final_plan",
        "scorer": "llm_judge",
        "config": {
          "provider": "openai",
          "model": "gpt-4o",
          "threshold": 0.8,
          "system_prompt": "You are the lead project manager. Review the entire final project plan for overall quality, coherence, and alignment with the original user goal. Look for any major gaps, inconsistencies, or repeated content. Return JSON with \"score\" (0-1) and \"reasoning\".\n",
          "user_prompt_template": "### Original User Goal:\n{metadata[user_goal]}\n### Final Revised Project Plan (JSON):\n{output}\n### Your Task: Provide a holistic, final verdict on the quality of the entire plan. Is it ready to be delivered?"
        },
        "on_fail": "continue",
        "span_kind": null
      }
    ],
    "batch_size": 10,
    "privacy_settings": {}
  },
  "model_configs": {
    "default_judge_config": {
      "provider": "openai",
      "model": "gpt-4.1",
      "temperature": 0.3,
      "max_tokens": 1000,
      "system_prompt": "You are an expert evaluator. Compare the actual output to the expected output and provide:\n1. A score from 0.0 to 1.0 (where 1.0 is perfect match)\n2. A brief reasoning for your score\n3. Any specific errors or discrepancies noted\n\nRespond in JSON format:\n{\n    \"score\": 0.0-1.0,\n    \"reasoning\": \"explanation\",\n    \"errors\": [\"error1\", \"error2\"] or []\n}"
    }
  },
  "selected_scorers": [],
  "timestamp": "20250719_032129"
}
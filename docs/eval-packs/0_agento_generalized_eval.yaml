schema_version: "1.0"
name: "Generalized Agento Lifecycle Evaluation"
version: "1.0"
description: >
  A reusable evaluation pack that judges the quality of each step in an Agento
  workflow by leveraging semantic OTEL attributes. Works on any Agento trace.

ingestion:
  type: "python"
  config:
    # These two keys are required by the PythonIngester to find our code
    script_path: "ingesters/agento_generalized_ingester.py"
    entry_function: "ingest_agento_trace"
    
    # This 'trace_file' key is a custom parameter our ingester function looks for.
    # In the UI, the uploaded file object will be passed as the value for this key.
    trace_file: "placeholder_for_ui_upload.otlp.json" 

pipeline:
  # This stage will ONLY run on items where metadata.step_type == 'plan'
  - name: "judge_initial_plan"
    scorer: "llm_judge"
    run_if: "metadata['step_type'] == 'plan'"
    config:
      provider: "openai"
      model: "gpt-4o"
      threshold: 0.8
      system_prompt: |
        You are an expert project manager. Evaluate if the provided project plan is a sound and comprehensive response to the user's goal. Return JSON with "score" (0-1) and "reasoning".
      user_prompt_template: |
        ### User's Goal:
        {input}

        ### Agent's Generated Plan:
        {output}

        ### Your Task: Evaluate the plan's quality and alignment with the goal.

  # This stage will ONLY run on DRAFT items
  - name: "judge_step_draft"
    scorer: "llm_judge"
    run_if: "metadata['step_type'] == 'draft'"
    config:
      provider: "openai"
      model: "gpt-4o"
      threshold: 0.7
      system_prompt: |
        You are an expert evaluator. Assess if the draft for step '{metadata[step_name]}' faithfully implements its instructions and criteria, considering the overall user goal. Return JSON with "score" and "reasoning".
      user_prompt_template: |
        ### Overall User Goal:
        {metadata[user_goal]}

        ### Instructions for '{metadata[step_name]}':
        {input}

        ### Evaluation Criteria for this Step:
        {expected_output}

        ### Agent's Draft for this Step:
        {output}

        ### Your Task: Evaluate how well the draft fulfills its specific instructions.

  # This stage will ONLY run on CRITIQUE items
  - name: "judge_step_critique"
    scorer: "llm_judge"
    run_if: "metadata['step_type'] == 'critique'"
    config:
      provider: "openai"
      model: "gpt-4o"
      threshold: 0.8
      system_prompt: |
        You are an expert at providing constructive feedback. Evaluate if the critique for '{metadata[step_name]}' is insightful, actionable, and likely to improve the draft to better meet the user's goal. Return JSON with "score" and "reasoning".
      user_prompt_template: |
        ### Overall User Goal:
        {metadata[user_goal]}
      
        ### Original Draft for '{metadata[step_name]}' (to be critiqued):
        {input}

        ### Agent's Critique of the Draft:
        {output}

        ### Your Task: Evaluate the quality of the critique. Is it valuable?
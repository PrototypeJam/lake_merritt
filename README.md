Below is a **concrete, high-level implementation plan**‚Äîdirectory tree first, then concise notes on the purpose and key contents of every file or sub-package.  This should give you everything you need to scaffold the repo and start filling in modules incrementally.

---

## 1‚ÄÉDirectory / File Layout

```
ai-eval-workbench/
‚îú‚îÄ README.md
‚îú‚îÄ pyproject.toml          # Poetry (or hatch) ‚Äì dep mgmt, tool config
‚îú‚îÄ requirements.txt        # fallback for pip-install
‚îú‚îÄ .env.template           # names of env vars (API keys etc.)
‚îú‚îÄ streamlit_app.py        # thin launcher ‚Üí pages/
‚îÇ
‚îú‚îÄ app/                    # Streamlit UI layer only
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ state.py             # helpers for st.session_state
‚îÇ  ‚îî‚îÄ pages/
‚îÇ     ‚îú‚îÄ 1_‚öôÔ∏è_config.py
‚îÇ     ‚îú‚îÄ 2_üìÑ_eval_setup.py
‚îÇ     ‚îú‚îÄ 3_üìä_results.py
‚îÇ     ‚îî‚îÄ 4_‚¨áÔ∏è_downloads.py
‚îÇ
‚îú‚îÄ core/                   # pure Python ‚Äúbusiness logic‚Äù
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ data_models.py       # Pydantic objects for everything
‚îÇ  ‚îú‚îÄ ingestion.py         # CSV ‚Üí List[EvalRecord]
‚îÇ  ‚îú‚îÄ generation.py        # model ‚Äúactor‚Äù output creation
‚îÇ  ‚îú‚îÄ evaluation.py        # orchestration: run_scorers()
‚îÇ  ‚îú‚îÄ reporting.py         # summary stats, JSON/CSV writers
‚îÇ  ‚îú‚îÄ logging_config.py
‚îÇ  ‚îî‚îÄ scoring/             # pluggable scorers live here
‚îÇ      ‚îú‚îÄ __init__.py
‚îÇ      ‚îú‚îÄ exact_match.py
‚îÇ      ‚îú‚îÄ fuzzy_match.py
‚îÇ      ‚îî‚îÄ llm_judge.py
‚îÇ
‚îú‚îÄ services/               # external integrations
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îî‚îÄ llm_clients.py       # OpenAI, Anthropic, Gemini wrappers
‚îÇ
‚îú‚îÄ utils/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ file_cache.py        # simple disk cache for rate-limit relief
‚îÇ  ‚îî‚îÄ telemetry.py         # placeholder OpenTelemetry hooks
‚îÇ
‚îú‚îÄ tests/
‚îÇ  ‚îú‚îÄ unit/
‚îÇ  ‚îÇ   ‚îî‚îÄ test_exact_match.py ‚Ä¶
‚îÇ  ‚îî‚îÄ integration/
‚îÇ      ‚îî‚îÄ test_end_to_end.py
‚îÇ
‚îî‚îÄ .github/
    ‚îî‚îÄ workflows/
        ‚îî‚îÄ ci.yml          # lint, unit tests
```

*(Emoji prefixes in `pages/` keep Streamlit tabs ordered.)*

---

## 2‚ÄÉModule Responsibilities

| Path                               | Core Responsibility                                                                                                    | Implementation Notes                                          |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |
| `streamlit_app.py`                 | `streamlit run` entrypoint. Imports `app.pages.*`; holds nothing else.                                                 | Keeps CLI simple and unopinionated.                           |
| **`app/state.py`**                 | Typed wrappers around `st.session_state` (config, uploaded data, results).                                             | Avoids raw string keys scattered across pages.                |
| **`app/pages/1_‚öôÔ∏è_config.py`**     | Page 1 UI: API keys, default judge model params. Writes to `state`.                                                    | Validate keys immediately with ping-call (optional).          |
| **`app/pages/2_üìÑ_eval_setup.py`** | Page 2 UI: Mode A vs B, file upload, scorer & actor selection, ‚ÄúStart Evaluation‚Äù.                                     | Delegates all heavy lifting to `core`.                        |
| **`app/pages/3_üìä_results.py`**    | Reads `state.results`; shows KPI cards, `st.dataframe`, expandable JSON reasoning.                                     | Charts via `st.altair_chart` or Plotly later.                 |
| **`app/pages/4_‚¨áÔ∏è_downloads.py`**  | Builds CSV/JSON bytes from `core.reporting`; exposes `st.download_button`.                                             | Future placeholders for logs/traces.                          |
| **`core/data_models.py`**          | Pydantic classes: `EvalRecord`, `Score`, `RunMetadata`, `RunResult`.                                                   | Single-source schema for I/O, scoring, reporting.             |
| **`core/ingestion.py`**            | Validates uploaded CSV, maps to `List[EvalRecord]`.                                                                    | Raises rich `pydantic.ValidationError` for UI display.        |
| **`core/generation.py`**           | For Mode B: loops through records, calls selected LLM client, fills `output`.                                          | Async aware; supports batch calls.                            |
| **`core/scoring/*`**               | One module per scorer. All expose `def score(record: EvalRecord, cfg: Any) -> Score`.                                  | Register in `scoring.__init__` for dynamic listing.           |
| **`core/evaluation.py`**           | `run_evaluation(records, scorer_cfgs) -> RunResult`. Handles concurrency, retries, logging.                            | Keeps Streamlit thread clear; progress reported via callback. |
| **`core/reporting.py`**            | Aggregate stats ‚Üí dict, plus `to_csv()` / `to_json()`.                                                                 | Consumed by UI & download page.                               |
| **`services/llm_clients.py`**      | Thin, typed wrappers around vendor SDKs. Standard interface: `generate(prompt, **params)`; `evaluate()` for judge LLM. | Centralizes retry logic, rate limits, exponential back-off.   |
| **`utils/file_cache.py`**          | Optional local caching for expensive LLM calls (dev mode).                                                             | Simple JSON-on-disk keyed by hash of call.                    |
| **`utils/telemetry.py`**           | Early placeholder to push OpenTelemetry spans.                                                                         | Keeps traces optional but path-ready.                         |
| **`logging_config.py`**            | Configures struct-log / standard logging for entire project.                                                           | Import first in `streamlit_app.py`.                           |
| **`tests/`**                       | Pytest suites. Unit tests for every scorer; integration test covers Mode A pipeline with fixtures.                     | CI fails fast on scoring regressions.                         |
| **CI workflow**                    | Lint (`ruff` + `mypy`), run tests.                                                                                     | Container step can later run Streamlit e2e with Playwright.   |

---

## 3‚ÄÉExtensibility & Future Features Hooks

* **New scorer drop-in:** put a `*.py` under `core/scoring/`, define `score()`, add to `__all__` list‚ÄîUI auto-picks it up because `scoring.list_scorers()` enumerates the modules.
* **Persisted runs & cross-run analytics:** `RunResult` already serializes cleanly; simply store JSON in `/runs/` or a DB.  A future page could load multiple `RunResult` files and feed them to a Plotly comparison view.
* **OpenTelemetry stream:** `utils.telemetry.trace_llm_call()` is invoked in `services.llm_clients.*`.  Switching to a real OTLP exporter later is configuration only.
* **API backend alternative:** If you later need a headless service, everything under `core/` is UI-agnostic.  Wrap it in FastAPI without touching Streamlit pages.

---

## 4‚ÄÉImmediate Next Steps

1. **Scaffold repo** with the tree above (`cookiecutter` or `copier` template).
2. Implement **data models** and **exact-match scorer** first‚Äîfastest path to an end-to-end ‚ÄúHello World‚Äù evaluation.
3. Add **fuzzy scorer** (pure Python `python-Levenshtein`).
4. Wire **Streamlit pages** minimally to ingest CSV and call `evaluation.run_evaluation`.
5. Layer in **LLM clients** and **LLM-judge scorer** once the plaintext path is solid.
6. Harden with **unit tests + CI** before tackling Mode B generation.

Feel free to ask for deeper dives on any module, detailed class signatures, or a cookiecutter template.
